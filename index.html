<!DOCTYPE html>
<!-- saved from url=(0039)https://phhei.github.io/ArgsValidNovel/ -->
<html lang="en-US">
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- Begin Jekyll SEO tag v2.8.0 -->
    <title>Welcome to the ImageArg official website</title>
    <meta name="generator" content="Jekyll v3.9.2">
    <meta property="og:title" content="The first shared-task in multimodal argument mining ">
    <meta property="og:locale" content="en_US">
    <meta name="description" content="The first shared-task in multimodal argument mining">
    <meta property="og:description" content="The first shared-task in multimodal argument mining">
    <link rel="canonical" href="https://imagearg.github.io/">
    <meta property="og:url" content="https://imagearg.github.io/">
    <meta property="og:site_name" content="https://imagearg.github.io/">
    <meta property="og:type" content="website">
    <meta name="twitter:card" content="summary">
    <meta property="twitter:title" content="The first shared-task in multimodal argument mining ">
    <script type="application/ld+json">
        {
            "@context": "https://schema.org",
            "@type": "WebSite",
            "description": "ImageArg Shared Task",
            "headline": "The first shared-task in multimodal argument mining",
            "name": "ImageArg Shared Task",
            "url": "https://imagearg.github.io/"
        }</script>
    <!-- End Jekyll SEO tag -->

    <link rel="stylesheet" href="./css/style.css">
    <!--[if lt IE 9]>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv.min.js"></script>
    <![endif]-->
    <!-- start custom head snippets, customize with your own _includes/head-custom.html file -->

    <!-- Setup Google Analytics -->


    <!-- You can set your favicon here -->
    <!-- link rel="shortcut icon" type="image/x-icon" href="/ArgsValidNovel/favicon.ico" -->

    <!-- end custom head snippets -->

    <style type="text/css">
        @font-face {
            font-weight: 400;
            font-style: normal;
            font-family: circular;

            src: url('chrome-extension://liecbddmkiiihnedobmlmillhodjkdmb/fonts/CircularXXWeb-Regular.woff2') format('woff2');
        }

        @font-face {
            font-weight: 500;
            font-style: normal;
            font-family: circular;

            src: url('chrome-extension://liecbddmkiiihnedobmlmillhodjkdmb/fonts/CircularXXWeb-Medium.woff2') format('woff2');
        }

        @font-face {
            font-weight: 700;
            font-style: normal;
            font-family: circular;

            src: url('chrome-extension://liecbddmkiiihnedobmlmillhodjkdmb/fonts/CircularXXWeb-Bold.woff2') format('woff2');
        }

        @font-face {
            font-weight: 900;
            font-style: normal;
            font-family: circular;

            src: url('chrome-extension://liecbddmkiiihnedobmlmillhodjkdmb/fonts/CircularXXWeb-Black.woff2') format('woff2');
        }</style>
</head>
<body>
<div class="wrapper">
    <header>
        <h1><a href="https://imagearg.github.io/">ImageArg-Shared-Task-2023</a></h1>


        <p>Co-located with <a href="https://argmining-org.github.io/2023/">The 10th ArgMining Workshop</a></a> at <a
                href="https://2023.emnlp.org/">EMNLP 2023</a> in Singapore. </p>
        <p class="view"><a href="https://github.com/ImageArg/ImageArg-Shared-Task">View Project Code on GitHub
            <small>ImageArg/ImageArg-Shared-Task</small></a></p>
        <p class="view"><a style="color: red">Important Announcement:</a><br>
            <b>[07/05]</b> Registration will close on 07/07 (Anywhere on Earth).<br>
            <b>[07/03]</b> Update data-downloading scripts. Please pull the
            <a href="https://github.com/ImageArg/ImageArg-Shared-Task"> new code</a>.<br>
            <b>[05/20]</b> To participate, please fill in this <a
                href="https://docs.google.com/forms/d/e/1FAIpQLSci3TSw6ylcWnjXQsoUjh3buAQx7IdgiJwrJDR2pDHMm8DIpQ/viewform?usp=pp_url">registration
            form</a>.
        </p>


    </header>
    <section>

        <h1 id="shared-task-on-predicting-validity-and-novelty-of-arguments">The First Shared Task in Multimodal
            Argument Mining</h1>

        <p>There has been a recent surge of interest in developing methods and corpora to improve and evaluate
            persuasiveness in natural language applications. However, these efforts have mainly focused on the textual
            modality, neglecting the influence of other modalities. <a
                    href="https://aclanthology.org/2022.argmining-1.1.pdf">Liu et al.</a> introduced a new multimodal
            dataset called <strong>ImageArg</strong>. This dataset includes persuasive tweets along with associated
            images, aiming to identify the image's stance towards the tweet and determine its persuasiveness score
            concerning a specific topic. The ImageArg dataset is a significant step towards advancing multimodal
            persuasive text analysis and opens up avenues for exploring the persuasive impact of images in social media.
            To further this goal, we designed this shared task, which utilizes the ImageArg dataset to advance
            multimodal persuasiveness techniques.</p>

        <p>Participants are welcome to submit system description papers for the shared task. Accepted papers will be
            published in the proceeding of <a
                    href="https://argmining-org.github.io/2023/">The 10th ArgMining Workshop</a>.

        To participate, please fill in this <a
                href="https://docs.google.com/forms/d/e/1FAIpQLSci3TSw6ylcWnjXQsoUjh3buAQx7IdgiJwrJDR2pDHMm8DIpQ/viewform?usp=pp_url">registration
            form</a> and feel free to join <a
                href="https://join.slack.com/t/imagearg/shared_invite/zt-1ss5hdb6d-eNCaWOAEe4O_8UE1gQxIxA">ImageArg
            Slack</a> for conversations.</p>



        <hr>

        <h2 id="dataset"> 1. The ImageArg Shared Task </h2>


        The ImageArg dataset is composed of tweets (images and text) from controversial topics,
        namely <strong> gun control </strong> and <strong> abortion</strong>. The the dataset
        contains 2-dimensions of annotations: Argumentative Stance (AS), Image Persuasiveness (IP), of which
        each dimension addresses a unique research question: 1) AS: does the tweet have an
        argumentative
        stance? 2) IP: does the tweet image make the tweet more persuasive?<p></p>

        <p>ImageArg Shared Task is divided into two subtasks. Participants can choose Task A or Task B, or both. Please
            be aware that some tweet content may be upsetting or triggering.</p>


        <h3 id="subtask-a-binary-novelty-validity-classification">Subtask A: Argumentative Stance (AS)
            Classification</h3>

        <p>Given a tweet composed of a text and image, predict whether the given tweet <strong> Supports </strong> or
            <strong> Opposes </strong> the
            given topic, which is a binary classification. Two examples are shown below. </p>

        <img src="example_stance.png" alt="Example of the <strong> AS </strong> classification">
        <p>The left tweets express strong stance towards support gun control by indicating a house bill about the
            requirement of background check of all gun sales. The right tweet opposes gun control because it is inclined
            to self-defense.</p>


        <!--        <p>Please read the <a href="https://github.com/MeiqiGuo/ArgMining2022-ImageArg/">Data Description</a>-->
        <!--            beforehand.</p>-->

        <!-- <blockquote>
          <p>If you use the data, please cite our <a href="https://aclanthology.org/">overview paper</a> and the <a href="https://aclanthology.org/">ImageArg paper</a> </p>
        </blockquote> -->
        <p></p>


        <h3 id="subtask-b-recognizing-relative-validity--novelty">Subtask B: Image Persuasiveness (IP)
            Classification</h3>

        <p>Given a tweet composed of text and image, predict whether the image makes the tweet text <strong>more
            Persuasive</strong> or <strong>Not</strong>, which is a binary classification task. Two examples are shown
            below.</p>

        <img src="example_persuasiveness.png" alt="Example of the <strong> IP </strong> classification">
        <p>The left tweet has an image not even relevant to gun control topic. It does not improve the
            persuasiveness of the left tweet that argues to focus on mental health instead of gun restriction. The tweet
            image
            on the right makes the tweet text (and its stance) more persuasive because it provides strong evidence to
            show the statistics of the murder
            rate in major U.S. cities due to restrict gun control laws, so citizens cannot easily arm themselves.</p>

        <p>Please read details about AS and IP in the <a
                href="https://aclanthology.org/2022.argmining-1.1.pdf">paper</a> if you
            are interested.</p>

        <hr>
        <h2 id="scripts to download the data"> 2. Dataset and Shared Task Submission</h2>
        <p>The dataset to download should only be used for participating in the ImageArg Shared Task. Any
            other use is explicitly prohibited. Participants are not allowed to redistribute the dataset per <a
                    href="https://developer.twitter.com/en/developer-terms/policy">Twitter
                Developer Policy</a>.</p>

        <p> All the tweets are instantly crawled from Twitter. Organizers are aware some tweets could not be available
            when
            participants start to download (e.g., a tweet could be deleted by its author). Organizers will regularly
            monitor the
            dataset to provide data patches that will replace invalid tweets with new annotated ones. Participants are
            required to fill out the <a
                    href="https://docs.google.com/forms/d/e/1FAIpQLSci3TSw6ylcWnjXQsoUjh3buAQx7IdgiJwrJDR2pDHMm8DIpQ/viewform">Google
                Form</a> in order to receive data patches and the shared task updates.</p>

        <p>Participants are allowed to extend only the training set with further (synthetic) samples. However, if do
            that, participants have to describe and the algorithm which extends the training set in the system
            description paper submission. This algorithm
            must be automatically executable without any human interaction (hence, without further manual
            annotation or manual user feedback).</p>

        <p><strong>Shared Task Evaluation:</strong> <strong> F1-score </strong> of participating teams
            will be used for ranking, but participants are free to include other metrics (e.g., AUC) in the system
            description paper submissions. </p>


        <p><strong>Shared Task Submission:</strong> There are up to 5 submissions from different approaches (systems)
            allowed per team and per
            subtask. Participants are allowed to withdraw your submission at anytime until the final
            deadline by contacting the organizers. </p>

        <ul>
            <li>Training and dev data download: <a href="https://github.com/ImageArg/ImageArg-Shared-Task">Here</a></li>
            <li>Test data (without labels) download: <a href="">TBA</a></li>
            <li>Evaluation script: <a href="">TBA</a></li>
            <li>Shared Task Submission: <a href="">TBA</a></li>
        </ul>


        <hr>

        <h2 id="system-description-paper-submission">3. Call for Papers (System Description Papers)</h2>

        <p>
            The ImageArg Shared Task invites the submission of system description papers from teams
            that have participated. Accepted papers will be published in the proceeding of <a
                href="https://argmining-org.github.io/2023/">The 10th ArgMining Workshop</a>.

        </p>

        <!--        <h3 id="system-description-paper-submission-information">Paper Submission Information</h3>-->
        <p>By default, we only accept short papers (at most 4 pages, including references and optional appendix). All
            papers will be treated equally in the workshop proceedings. Moreover, authors are expected to adhere to the
            ethical code set out in the ACL Code of Ethics. Submissions that violate any of the policies will be
            rejected without review.

        <p>Please use the <a href="https://2023.emnlp.org/calls/style-and-formatting/">EMNLP 2023 style sheets</a> for
            formatting your paper. </p>

        <p><strong>Paper Submission:</strong> address is to be announced.</p>

        <!--Please use the CAMERA-READY, non-anonymized option.-->

        <!--        <h3 id="paper-guidance">Suggested Paper Structure</h3>-->


        <hr>
        <h2 id="timeline">4. Timeline</h2>
        <!--        The timeline for shared task and system description paper submissions.-->
        <ul>
            <li>05/15/23: Training and Dev scripts (data) released for both subtasks</li>
            <li>07/07/23: Registration closed and Test scripts released for both subtasks</li>
            <li>07/21/23: Shared task submission due.</li>
            <li>08/01/23: Leaderboard announcement for both tasks</li>
            <li>09/01/23: System description paper due</li>
            <li>10/02/23: Notification of paper acceptance</li>
            <li>10/09/23: Camera-ready paper due</li>
            <li>12/10/23: Workshop dates</li>
        </ul>
        <hr>
        <h2 id="terms-and-conditions">5. Terms and Conditions</h2>

        <p>By participating in this task you agree to these terms and conditions. If, however, one or more of these
            conditions is a concern for you, email us, and we will consider if an exception can be made.</p>

        <ul>
            <li>By submitting results to this competition, you consent to the public release of your scores at this
                website and at ArgMining-2023 workshop and in the associated proceedings, at the task organizers’
                discretion. Scores may include, but are not limited to, automatic and manual quantitative judgements,
                qualitative judgements, and such other metrics as the task organizers see fit. You accept that the
                ultimate decision of metric choice and score value is that of the task organizers.
            </li>
            <li>You further agree that the task organizers are under no obligation to release scores and that scores may
                be withheld if it is the task organizers’ judgement that the submission was incomplete, erroneous,
                deceptive, or violated the letter or spirit of the competition’s rules. Inclusion of a submission’s
                scores is not an endorsement of a team or individual’s submission, system, or science.
            </li>
            <li>A participant can be involved in one team. Participating in more than one team is not recommended, but
                not forbidden (if the person does not apply the same approach in different teams)
                <ul>
                    <li>You must not use any data from the development split as training instances. You must not use any
                        test instance in the training of the model (also not indirectly for model selection). Approaches
                        that violate this data separation are disqualified.
                    </li>
                    <li>The usage of Large Language Models LLMs are permitted as long as they are freely available (eg.
                        LLAMA). Paid APIs for LLMs for (eg. openAI API) will be scored and reported but will not be
                        considered for final ranking to ensure fairness between teams who might not have these APIs
                        available.
                    </li>
                </ul>
            </li>
            <li>Once the competition is over, we will release the gold labels, and you will be able to determine results
                on various system variants you may have developed. We encourage you to report results on all of your
                systems (or system variants) in the system-description paper. However, we will ask you to clearly
                indicate the result of your official submission.
                <ul>
                    <li>We will make the final submissions of the teams public at some point after the evaluation
                        period.
                    </li>
                    <li>The organizers and their affiliated institutions makes no warranties regarding the datasets
                        provided, including but not limited to being correct or complete. They cannot be held liable for
                        providing access to the datasets or the usage of the datasets.
                    </li>
                    <li>The dataset should only be used for scientific or research purposes. Any other use is explicitly
                        prohibited.
                    </li>
                    <li>The datasets must not be redistributed or shared in part or full with any third party. Redirect
                        interested parties to this website.
                    </li>
                </ul>
            </li>
        </ul>

        <hr>
        <h2 id="task-organizers">6. Shared Task Organizers</h2>


        <ul>

            <!--                <ul>-->
            <li><a href="https://people.cs.pitt.edu/~zhexiong/">Zheixong Liu</a>, Ph.D. Student of Computer
                Science
            </li>
            <li><a href="https://engsalem.github.io/">Mohamed Elaraby</a>, Ph.D. Student of Computer Science
            </li>
            <li><a href="http://yangzhongcs.com/">Yang Zhong</a>, Ph.D. Student of Computer Science</li>
            <li><a href="https://people.cs.pitt.edu/~litman/">Diane Litman</a>, Professor of Computer Science
            </li>
            <li><strong>Contact</strong>: imagearg [at] gmail.com</li>
            <!--                </ul>-->
            <li><strong>Affiliation</strong>: University of Pittsburgh</li>
        </ul>


    </section>
    <footer>

        <!--        <p>This project is maintained by <a href="https://github.com/ImageArg">ImageArg</a></p>-->
        <p>This website is developed based on and partially reusing <a href="https://phhei.github.io/ArgsValidNovel/">ArgMining22-Shared-Task
        </a></p>
        <p><small>Hosted on GitHub Pages — Theme by <a href="https://github.com/orderedlist">orderedlist</a></small></p>

    </footer>
</div>
<script src="./js/scale.fix.js"></script>


<div id="loom-companion-mv3" ext-id="liecbddmkiiihnedobmlmillhodjkdmb">
    <section id="shadow-host-companion"></section>
</div>
</body>
</html>
